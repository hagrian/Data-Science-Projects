# -*- coding: utf-8 -*-
"""Team1_CourseProject_IST718BDA_Exploratory_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FYJoGkilvAO2FX_TtHnXkJFGnhGmlWVP

## IST 718 BDA Final  Project


# Russian Troll Tweets Analysis
**Identifying Russian Troll Accounts on Twitter**

**Team 1**
* Adil Gokturk
* Drew Howell
* Scott Snow

## Story &  Hypothesis 


 According to the House Intelligence Committee investigation the, Russia’s Internet Research Agency attempted to interfere with the 2016 U.S. election by running fake accounts on Twitter, know as “Russian trolls”.

* Would it be possible to demonstrate whether these tweets used to manipulate/target the 2016 U.S. election to favor on any presidential candidate?

* Would it be possible by developing machine learning models to predict whether a Twitter account is a Russian troll/fake?

##  Exploratory analysis 

**Load the Libraries**
"""

import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('seaborn-darkgrid')
import numpy as np
# %matplotlib inline  
# %pylab inline
import re
re.compile('<title>(.*)</title>') # regular expression

import seaborn as sns

"""**Read the Data Set, "tweets.csv"**"""

tweets = pd.read_csv('tweets.csv')
print(tweets.shape)
tweets.head(2)

df = pd.read_csv("https://raw.githubusercontent.com/ssnow5516/my_files/master/edtwts0.csv")
srt_url = "https://raw.githubusercontent.com/ssnow5516/my_files/master/edtwts"
for i in np.arange(1,9,1):
    this_gts = pd.read_csv(srt_url + str(i) + ".csv")
    df = pd.concat([df, this_gts])

"""Let's check the missing values in the df"""

tweets.isnull().sum().sort_values(ascending = False)

"""There are 21 NaNs in the text.

Let's drop the NaNs in the text colum
"""

tweets.dropna(subset=['text'], inplace=True)

"""Let's check the data types of the columns and convert  "created string" to "datetime""""

print(tweets.dtypes)

"""Convert created_str to datetime format"""

tweets['created_str'] = pd.to_datetime(tweets['created_str'])

"""Convert ids to object datatype"""

columns = ['user_id', 'tweet_id', 'retweeted_status_id', 
           'retweeted_status_id', 'in_reply_to_status_id']

for column in columns :
  tweets[column] = tweets[column].astype('object')

# check the data types again
tweets.dtypes

"""**Check the period of the tweets**"""

start_date_tweet = tweets['created_str'].min()
end_date_tweet = tweets['created_str'].max()

print(start_date_tweet, end_date_tweet)

"""The data sets have   about 3 years of tweets starting 14th July 2014 until the 26th of September 2017.

Times appear with these dates, so let's create a new column to hold only the date component of this.
"""

tweets['created_str_date'] = pd.to_datetime(tweets['created_str'].dt.date)

"""Let' s take a look at the trend of these tweets against time"""

# Count the number of times a date appears in the dataset and convert to dataframe
tweet_trend = pd.DataFrame(tweets['created_str_date'].value_counts())

# index is date, columns indicate tweet count on that day
tweet_trend.columns = ['tweet_count']

# sort the dataframe by the dates to have them in order
tweet_trend.sort_index(ascending = True, inplace = True)

#make a line plot of the tweet count data and give some pretty labels! ;)
# the 'rot' argument control x-axis ticks rotation
plt.style.use('seaborn-darkgrid')
tweet_trend['tweet_count'].plot(linestyle = "-", figsize = (12,8), rot = 45, color = 'k',
                               linewidth = 1)
plt.title('Tweet counts by date', fontsize = 15)
plt.xlabel('Date', fontsize = 13)
plt.ylabel('Tweet Count', fontsize = 13)

"""If these tweets really had to impact the way of the US presidential elections, these tweets would be most numerous during important parts of the trump rally or milestones in the trump rally. 

Let's get these dates online and try to map that data on with the line plot!
"""

# these are dates corresponding to important dates from the trump campaign.
dates_list = ['2015-06-16', '2015-12-07', '2016-02-01',
              '2016-03-01', '2016-03-03', '2016-03-11',
              '2016-05-03', '2016-05-26', '2016-06-20', 
              '2016-07-15', '2016-07-21', '2016-08-17',
              '2016-09-01', '2016-10-07', '2016-11-08']

# create a series of these dates.
important_dates = pd.Series(pd.to_datetime(dates_list))

# add columns to identify important events, and mark a 0 or 1.
tweet_trend['Important Events'] = False
tweet_trend.loc[important_dates, 'Important Events'] = True
tweet_trend['values'] = 0
tweet_trend.loc[important_dates, 'values'] = 1

"""Let's plot the line chart for trend, a monthly average of tweet counts and add red dots to mark important events."""

plt.style.use('seaborn-darkgrid')
tweet_trend['tweet_count'].plot(linestyle = "--", 
                                figsize = (12,8), rot = 45, 
                                color = 'k',
                                label = 'Tweet Count per Day',
                               linewidth = 1)

# plot dots for where values in the tweet_trend df are 1
plt.plot(tweet_trend[tweet_trend['Important Events'] == True].index.values,
         tweet_trend.loc[tweet_trend['Important Events'] == True, 'values'],
         marker = 'o', 
         color = 'r',
         linestyle = 'none',
        label = 'Important Dates in campaign')

# Lets add a 30 day moving average on top to view the trend! Min_periods tells rolling() to
# use 10 points if 30 not available!
plt.plot(tweet_trend['tweet_count'].rolling(window = 30, min_periods = 10).mean(), 
         color = 'r', 
         label = '30 Day Moving Avg # of tweets')
plt.title('Tweet counts by date', fontsize = 15)
plt.xlabel('Date', fontsize = 13)
plt.ylabel('Tweet Count', fontsize = 13)
plt.legend(loc = 'best')

"""### The US president was elected on 8th November 2016 - the last red dot on the chart. 

### The tweet activity near the end of the campaign was  increased.

### Let's  Calculate the percentage change in Tweets counts
"""

tweet_trend['Pct_Chg_tweets'] = tweet_trend['tweet_count'].pct_change()*100

# Lets see values only for the important dates. This Pct_Chg_tweets shows us the percentage
# change in tweets for the day of the event versus the previous day!
tweet_trend.loc[tweet_trend['values'] == 1,['tweet_count', 'Pct_Chg_tweets']]

"""Evidently, for most of the dates related to the Trump campaign, **there is an increasing trend in the tweet counts,** sometimes as large as 1700%. 

Let's plot the percentage change
"""

# line plot of the % change in tweets counts
tweet_trend['Pct_Chg_tweets'].plot(linestyle = "--",
                                   figsize = (12, 8), rot = 45,
                                  color = 'k',
                                  linewidth = 1)

# Let's add the dots for important events
plt.plot(tweet_trend[tweet_trend['Important Events'] == True].index.values,
        tweet_trend.loc[tweet_trend['Important Events'] == True, 'values'],
        marker = 'o',
        color = 'r',
        linestyle = 'none')
plt.title('Tweet Counts  Change', fontsize = 17)
plt.xlabel('Date', fontsize = 15)
plt.ylabel('Tweet Counts  Change', fontsize = 15)

"""### Text Analytics

Tweets data set contains some  additional features such as  RT mentions, links, hastags etc. 

Let's seperate our these features to analyze just the tweet text or hastags
"""

# text column features
tweets['text'].head(10)

"""Apparently; 
* Retweets begin with the keyword 'RT'. These are followed by @userkey.
* Hashtags begin with a # and are one continuous string with a space next to them
* Links begin with https:// or http:// and can be present anywhere in the string.
* User mentions begin with '@' 


**Let's remove the RT mentions from tweets** by creating a function.
"""

def remove_retweet(tweet):
  just_text = []
  if len(re.findall("^RT.*?:(.*)", tweet)) > 0:
    just_text.append(re.findall("^RT.*?:(.*)", tweet)[0])
  else:
    just_text.append(tweet)
  return just_text[0]

"""Extract texts and place in a list"""

just_text = tweets['text'].map(remove_retweet)

"""**Remove the links form the tweets**"""

def remove_links(tweet):
  just_text = []
  if len (re.findall("(https://[^\s]+)", tweet)) > 0:
    tweet = re.sub("(https://[^\s]+)","", tweet)
  if len(re.findall("(https://[^\s]+)", tweet)) > 0:
    tweet = re.sub("(https://[^\s]+)", "", tweet)
  just_text.append(tweet)
  return just_text[0]

text_no_links = just_text.map(remove_links)

"""**Remove the Hashtags**"""

def remove_hashtags(tweet):
    hashtags_only = []
    if len(re.findall("(#[^#\s]+)", tweet)) > 0:
        tweet = re.sub("(#[^#\s]+)", "", tweet) 
    hashtags_only.append(tweet)
    return hashtags_only[0]

text_all_removed = text_no_links.map(remove_hashtags)

"""Remove all extra spaces"""

def remove_extraneous(tweet):
    tweet = tweet.rstrip()
    tweet = tweet.lstrip()
    tweet = tweet.rstrip(")")
    tweet = tweet.lstrip("(")
    tweet = re.sub("\.", "", tweet)
    return tweet

text_clean = text_all_removed.map(remove_extraneous)

"""Let's remove the user mentions from the cleaned text"""

def extract_mentions(tweet):
    mentions = []
    if len(re.findall('@[^\s@]+', tweet))>0:
        mentions.append(re.findall('@([^\s@]+)', tweet))
    else:
        mentions.append(["0"])
    return mentions[0]
 
# Add to user mentions in a new column in df 
tweets['user_mentions'] = text_clean.map(extract_mentions)

# Remove the mentions form the text

def remove_mentions(tweet):
    mentions = []
    if len(re.findall('@[^\s@]+', tweet))>0:
        tweet = re.sub('@[^\s@]+', "" , tweet)
        mentions.append(tweet)
    else:
        mentions.append(tweet)
    return mentions[0]

text_clean_final = text_clean.map(remove_mentions)

"""Let's append the new clean text list to df"""

tweets['tweet_text_only'] = text_clean_final

"""### The most used hastags list"""

def extract_hashtags(tweet):
    hashtags_only = []
    if len(re.findall("(#[^#\s]+)", tweet)) > 0:
        hashtags_only.append(re.findall("(#[^#\s]+)", tweet))
    else:
        hashtags_only.append(["0"])
    return hashtags_only[0]

# let's add to the df and view the first 10
tweets['tweet_hashtags'] = tweets['text'].map(extract_hashtags)
tweets['tweet_hashtags'].head(10)

"""Create a list of Hashtags"""

all_hashtags = tweets['tweet_hashtags'].tolist()

cleaned_hashtags = []
for i in all_hashtags:
  for j in i:
    cleaned_hashtags.append(j)

# convert cleaned to a series to count the most frequently accuring
cleaned_hashtag_series = pd.Series(cleaned_hashtags)
hashtag_counts = cleaned_hashtag_series.value_counts()

"""Let's create a **Word Cloud** to visualize the most commonly used hashtags.

let's put the word list as a single string separated by spaces, first.
"""

hashes = cleaned_hashtag_series.values
hashes = hashes.tolist()

# convert list to one string with all the words
hashes_words = " ".join(hashes)

# generate the wordcloud. 
from wordcloud import WordCloud
wordcloud = WordCloud(width= 1600, height = 800, 
                      relative_scaling = 1.0, 
                      colormap = "Blues",
                     max_words = 100).generate(hashes_words)

plt.figure(figsize=(15,10))
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""# **Top 20 Hashtags used in Troll tweets**"""

plt.style.use('seaborn-darkgrid')
plt.figure(figsize = (12,8))
plt.barh(y = hashtag_counts[1:21].index.values, width = hashtag_counts[1:21])
plt.title("Top 20 Hashtags used in Troll tweets", fontsize = 15)
plt.xlabel('Count of hashtags', fontsize = 13)
plt.ylabel('Hashtags', fontsize = 13)

"""The most common hashtags:



1.   #POLITICS
2.   #TCOT
3.   #MAGA
4.   #PJNET
5.   #news
6.   #Trump

Apparenty, the trolls highly interted to the current president's campaign.


### Were these hashtags used most before the president's campaign?

Let's use the top 6 hashtags and a count of how many times these were used on the dates provided in the created_str_date.
"""

# Create a new df by using the date and the hashtags on that date
hashtag_date_df = tweets[['created_str_date','tweet_hashtags']]
hashtag_date_df = hashtag_date_df.reset_index(drop=True)

# extract a list of hashtags from the df
all_hashtags = hashtag_date_df['tweet_hashtags'].tolist()

hashtag_date_df.head(10)

"""For the top 6 hashtags, lets calculate how many times that appears against each date"""

count_dict = {}
for i in hashtag_counts.index.values[1:7]:
  count_hash = []
  for j in all_hashtags:
    count_hash.append(j.count(i))
  count_dict[i] = count_hash

"""Create a hashtag data frame"""

hashtag_count_df = pd.DataFrame(count_dict)

# concatebate the new df with the hashtag_count_df
hashtag_count_df = pd.concat([hashtag_date_df, hashtag_count_df], axis=1)

hashtag_count_df.head(10)

"""### Let's summarize the data to see monthly usage of the most common hashtags"""

# change the created_str column into datetime format and extract just the date from it
hashtag_count_df['created_str_date'] = pd.to_datetime(hashtag_count_df['created_str_date'])

# set the index so as to plot the time series
hashtag_count_df.set_index('created_str_date', inplace = True)

# get a monthly sum of the tweets for each of these hashtags
hashtag_count_df_pivot = hashtag_count_df.resample('M').sum()

# replace 0 with NaN so that these can be removed in rows where they are all NaNs
hashtag_count_df_pivot.replace(0, np.nan, inplace = True)
hashtag_count_df_pivot.dropna(how = 'all', inplace = True, axis = 0)

# replace NaNs back by 0s so that we can plot
hashtag_count_df_pivot.replace(np.nan, 0, inplace = True)
hashtag_count_df_pivot

"""Let's plot the montly usage of the most common hashtags"""

plt.style.use('seaborn-darkgrid')
# create a 3 by 2 subplot to hold the trend of all hashtags
figure, ((ax1, ax2), (ax3, ax4), (ax5, ax6)) = subplots(nrows = 3,
                                                       ncols = 2,
                                                       sharey = True,
                                                       figsize = (10,8))

plt.subplots_adjust(top = 1, hspace = 0.9)
hashtag_count_df_pivot['#politics'].plot(linestyle = "-", marker = "o", color = "green",ax = ax1)
ax1.set_title("#POLITICS", fontsize = 10)
ax1.set_xlabel('Date', fontsize = 12)

hashtag_count_df_pivot['#tcot'].plot(linestyle = "-", marker = "o", color = "red", ax = ax2)
ax2.set_title("#TCOT", fontsize = 10)
ax2.set_xlabel('Date', fontsize = 12)

hashtag_count_df_pivot['#MAGA'].plot(linestyle = "-", marker = "o", color = "orange", ax = ax3)
ax3.set_title("#MAGA", fontsize = 10)
ax3.set_xlabel('Date', fontsize = 12)

hashtag_count_df_pivot['#PJNET'].plot(linestyle = "-", marker = "o", color = "blue",ax = ax4)
ax4.set_title("#PJNET", fontsize = 10)
ax4.set_xlabel('Date', fontsize = 12)

hashtag_count_df_pivot['#news'].plot(linestyle = "-", marker = "o", color = "grey", ax = ax5)
ax5.set_title("#NEWS", fontsize = 10)
ax5.set_xlabel('Date', fontsize = 12)

hashtag_count_df_pivot['#Trump'].plot(linestyle = "-", marker = "o", color = "maroon", ax = ax6)
ax6.set_title("#TRUMP", fontsize = 10)
ax6.set_xlabel('Date', fontsize = 12)

"""#### Apparently, most of these hashtags picked up in the year 2016 near March or later in July, close to the elections.

### User mentions

Let's create a df with each user's tweet with the user mentions against it
"""

tweets['user_mentions'].head(10)

user_mention = tweets.loc[:,['user_key', 'user_mentions']]

user_mention.head(10)

"""Let's remove the rows where no user is mentioned"""

row_remove_mask = user_mention['user_mentions'].map(lambda x: "0" in x)

# number of supposed to be removed 
np.sum(row_remove_mask)

# let's keep the rows where row_remove_mask is False
user_mention_df = user_mention.loc[~row_remove_mask, :]
user_mention_df.reset_index(drop = True, inplace = True)

user_mention_df.head(10)

"""This gives us each user and the user they mentioned. For meaningful analysis,  each row needs to have a user against one user and not multiple."""

# for each row, create a one-to-one tuple of user and his user mention
new_list = []
for i in range(len(user_mention_df)):
    for j in user_mention_df.loc[i, "user_mentions"]:
        (a,b) = (user_mention_df.loc[i, 'user_key'], j)
        new_list.append((a,b))

# get the df
user_mention_clean_df = pd.DataFrame({"User_Key": [a for (a,b) in new_list],
                                     "User_Mention": [b for (a, b) in new_list]})

user_mention_clean_df.head(10)

"""## Clustering by HashTags

Let's create a df with user and hashtags in one tweet
"""

user_hashtag_df =tweets[['user_key', 'tweet_hashtags']]
user_hashtag_df = user_hashtag_df.reset_index(drop = True)

"""remove the rows where no hashtags were used"""

row_remove_mask = user_hashtag_df['tweet_hashtags'].map(lambda x: "0" in x)

# remove the rows
user_hashtag_df_clean = user_hashtag_df.loc[~row_remove_mask,:]
user_hashtag_df_clean.reset_index(drop=True, inplace=True)

user_hashtag_df_clean.head(10)

"""Let's separate out all hashtags used"""

all_hashtags = user_hashtag_df_clean['tweet_hashtags']

"""Let's get hashtags that qualify - present in 50 or more tweets"""

qualify_hashtags_mask = (hashtag_counts >= 50)
qualify_hashtags = hashtag_counts[qualify_hashtags_mask]

# remove the "0" hashtags
qualify_hashtags = qualify_hashtags.drop(labels = "0")

qualify_hashtags.head(20)

len(qualify_hashtags)

"""### The qualify_hashtags has 435 hashtags that are present in 50 or more different tweets. It would be  28000 unique hashtags without the limit of 50>!"""

# lets count the number of times these qualified hashtags appear in the tweets with hashtags
count_dict = {}

for i in qualify_hashtags.index.values:
  count_hash = all_hashtags.map(lambda x: x.count(i))
  count_dict[i] = count_hash

# create a dataframe from the hashtags and their counts in tweets
hashtag_count_df = pd.DataFrame(count_dict)

# concatenate this dataframe with the hashtag_count_df
user_hashtag_count_df = pd.concat([user_hashtag_df_clean, hashtag_count_df], axis = 1)

# group by user_key and get the sum of times they have used a hashtag
user_hashtag_group = user_hashtag_count_df.groupby('user_key').agg('sum').reset_index()

user_hashtag_group.head(5)

"""### The user wise tweet text"""

user_tweet_df = tweets.loc[:,['user_key', 'tweet_text_only']]

user_tweet_df.head(10)

"""## Users data set (users.csv)
Let's look at the users who wrote the tweets
"""

users = pd.read_csv('users.csv')
users.head()

"""### Users' location and language

 Let's get a count of users from each time-zone and language combination
"""

user_loc_lang = users.groupby(['time_zone', 'lang'])['id'].agg('count').reset_index()

user_loc_lang.rename(columns = {'id':'user_count'}, inplace = True)

user_loc_lang.head()

"""Lets create a sankey plot to get a sense of **which time zone are the users from and what language they speak**"""

!pip install pySankey
from pySankey import sankey

sankey.sankey(user_loc_lang['time_zone'],
             user_loc_lang['lang'],
             leftWeight = user_loc_lang['user_count'],
             rightWeight = user_loc_lang['user_count'],
             fontsize = 8)
plt.title("Users profile in the troll tweet users")

"""Apparently, **English speaking users come from US, Canada & Arizona.** 
 
**Russian speaking users come from Moscow, Volgograd, Yerevan and Minsk.** 

#### All french speaking users are from Paris. 

## Twitter Accounts creation date

The created_at column in the users dataframe provides this information
"""

# create a df
users['created_at'] = pd.to_datetime(users['created_at'])
users['created_at_date'] = pd.to_datetime(users['created_at'])

users['created_at_date'].head()

"""Plot the Troll User Accounts Creation date"""

user_created = users.groupby('created_at_date')['id'].agg('count')

plt.style.use('fivethirtyeight')
user_created.resample('W', kind = 'period').sum().\
plot(linestyle = '-', figsize = (10,8), linewidth = 1)
title('Troll User Account Created')
xlabel('Dates')
ylabel('Count of accounts created')

"""### The most tweeted users"""

user_tweet_count =tweets.groupby('user_id')['text'].agg('count').reset_index()
user_tweet_count.rename(columns = {'text': 'Tweet_count'}, inplace = True)

# merge the df with the users data
user_tweet_count_df = user_tweet_count.merge(users,
                                            left_on = 'user_id',
                                            right_on = 'id')

user_tweet_count_df.head(10)

"""Plot the top 10 tweeting Users"""

plt.style.use('seaborn-darkgrid')
user_tweet_count_df[['name', 'Tweet_count']].sort_values('Tweet_count', ascending = False)[:10].\
set_index('name').plot(kind = 'barh', figsize = (10,8))
title('User wise Tweet Count', fontsize = 15)
xlabel('Tweet Count', fontsize = 13)
ylabel('User Name', fontsize = 13)

"""### Is there a correlation between higher number of followers and larger number of tweets?"""

correl = user_tweet_count_df['Tweet_count'].corr(user_tweet_count_df['followers_count'])
print("{0:.2f}".format(correl))

# Let's draw a scatterplot of the tweet count with the number of followers
fig = plt.figure(figsize = (10,8))
plt.style.use('seaborn-darkgrid')
plt.scatter(user_tweet_count_df['Tweet_count'], 
        user_tweet_count_df['followers_count'],
       marker = 'o',
       alpha = 0.5)
plt.title("Followers vs Number of Tweets", fontsize = 15)
plt.xlabel("Number of Tweets", fontsize = 10)
plt.ylabel("Follower Count", fontsize = 10)
plt.text(6000, 80000, s = "Correlation is: {0:.2f}".format(correl), fontsize = 15)

"""### No such  correlation exists.

 Apparently, most users have very low tweet counts but their followers range from very few to numerous
 
 ## Users' registered Languages
"""

user_tweet_count_df['lang'].value_counts()

"""### English is the most common language, followed by Russian and German

## The most influential Users
"""

user_tweet_count_df[['name','lang','followers_count']].sort_values('followers_count', ascending = False)[:10]

"""### Let's save the files as datasets"""

user_mention_clean_df.to_csv('User_Mentions.csv')
user_hashtag_group.to_csv('User_Hashtags.csv')
user_tweet_df.to_csv('User_Tweets.csv')

"""## Machine Learning Applications


### Keras Model

#### Load the Libraries
"""

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

"""### Let's create a simple Keras DF"""

def my_analyzer(lot, grams=1):
    if grams < 1:
        return None
    lot = remove_retweet(lot)
    lot = remove_hashtags(lot)
    lot = remove_links(lot)
    lot = remove_extraneous(lot)
    lot = remove_mentions(lot)
    tokens = lot.split(" ")
    newwords = []
    for word in tokens:
        if len(word) > 0 and word not in stop_words:
            newwords.append(word)
    if len(newwords) < grams:
        return ['notext']
    if grams == 1:
        return newwords
    ngrams = []
    for i, words in enumerate(newwords):
        this_gram = []
        if i < len(newwords)- grams + 1:
            for j in range(grams):
                this_gram.append(newwords[i+j])
            ngrams.append(tuple(this_gram))
    return(ngrams)

k_tweets = tweets.loc[:,('text','created_at')]
k_tweets['troll'] = 1

good_twts = df.loc[:,('text','created_at')]
good_twts['troll'] = 0

keras_df = pd.concat([k_tweets, good_twts])
keras_df.reset_index(inplace=True, drop=True)

"""Load sklearn  Count Vectorizer

### Create Train/test Split
"""

from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(analyzer=my_analyzer, ngram_range=(1,3), binary=False, min_df=5, stop_words='english')

X = keras_df.loc[:,('text', 'created_at')]
y = keras_df.loc[:,'troll']

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=None)
X_train, X_test1, y_train, y_test1 = train_test_split(X_train, y_train, test_size=0.5, random_state=None)
X_train, X_test2, y_train, y_test2 = train_test_split(X_train, y_train, test_size=0.5, random_state=None)
X_train, X_test3, y_train, y_test3 = train_test_split(X_train, y_train, test_size=0.5, random_state=None)

X_train_vec = vectorizer.fit_transform(X_train['text'])
X_test_vec = vectorizer.transform(X_test['text'])
X_test_vec1 = vectorizer.transform(X_test1['text'])
X_test_vec2 = vectorizer.transform(X_test2['text'])
X_test_vec3 = vectorizer.transform(X_test3['text'])

# Let's take a look at the train/test vec dimentions and model keras

print(X_train_vec.shape)
print(X_test_vec3.shape)

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout

num_words = X_train_vec.shape[1]

def baseline_model():
    # create model
    model = Sequential()
    model.add(Dense(num_words, input_dim=num_words, kernel_initializer='normal', activation='relu'))
    model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))
    # Compile model
    model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])
    return model

"""### Evaluate the model"""

from keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10)
model2 = baseline_model()
history  = model2.fit(X_train_vec, y_train, validation_data=(X_test_vec3, y_test3), epochs=16, batch_size=10000, verbose=2, callbacks=[early_stopping])

scores = model2.evaluate(X_test_vec3, y_test3, verbose=0)

model2.save("model.h5")

print(scores)
y_pred = model2.predict(X_test_vec3)

"""### Let's plot the Training and Validation Accuracies of the model"""

def plot_train_curve(history):
    colors = ['#e66101','#fdb863','#b2abd2','#5e3c99']
    accuracy = history.history['acc']
    val_accuracy = history.history['val_acc']
    loss = history.history['loss']
    val_loss = history.history['val_loss']
    epochs = range(len(accuracy))
    with plt.style.context("ggplot"):
        plt.figure(figsize=(8, 8/1.618))
        plt.plot(epochs, accuracy, marker='o', c=colors[3], label='Training accuracy')
        plt.plot(epochs, val_accuracy, c=colors[0], label='Validation accuracy')
        plt.title('Training and validation accuracy')
        plt.legend()
        plt.figure(figsize=(8, 8/1.618))
        plt.plot(epochs, loss, marker='o', c=colors[3], label='Training loss')
        plt.plot(epochs, val_loss, c=colors[0], label='Validation loss')
        plt.title('Training and validation loss')
        plt.legend()
        plt.show()
      
plot_train_curve(history)

y_pred = [int(np.round(y_pred[x])) for x in range(len(y_pred))]
#y_pred2 = [int(np.round(y_pred2[x])) for x in range(len(y_pred2))]

"""### Confusion Matrix"""

from sklearn.metrics import confusion_matrix
cm1 = confusion_matrix(y_test3, y_pred, labels=[0,1])
print(cm1)

from sklearn.metrics import classification_report
print(classification_report(y_test3,y_pred, target_names = ['Real','Troll']))

#cm2 = confusion_matrix(y_test2, y_pred2, labels=[0,1])
#print(cm2)

#print(classification_report(y_test2,y_pred2, target_names = ['Real','Troll']))

